
@book{russell_artificial_2016,
	address = {Boston Columbus Indianapolis},
	edition = {Third edition, Global edition},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Artificial intelligence: a modern approach},
	isbn = {9780136042594 9781292153971},
	shorttitle = {Artificial intelligence},
	language = {eng},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	collaborator = {Davis, Ernest and Edwards, Douglas},
	year = {2016},
}

@book{weiss_multiagent_2001,
	address = {Cambridge, Mass.},
	edition = {3. print},
	title = {Multiagent systems: a modern approach to distributed artificial intelligence},
	isbn = {9780262731317 9780262232036},
	shorttitle = {Multiagent systems},
	language = {eng},
	publisher = {MIT Press},
	editor = {Weiss, Gerhard},
	year = {2001},
}

@book{boden_artificial_2018,
	address = {[Oxford]},
	series = {Very short introductions},
	title = {Artificial intelligence: a very short introduction},
	isbn = {9780191821448},
	shorttitle = {Artificial intelligence},
	abstract = {The applications of Artificial Intelligence lie all around us: in our homes, schools and offices, in our cinemas, in art galleries and - not least - on the Internet. Its results have been invaluable to biologists, psychologists, and linguists in helping to understand the processes of memory, learning, and language from a fresh angle.0As a concept, Artificial Intelligence has fuelled and sharpened the philosophical debates concerning the nature of the mind, intelligence, and the uniqueness of human beings. Margaret A. Boden reviews the philosophical and technological challenges raised by Artificial Intelligence, considering whether programs could ever be really intelligent, creative or even conscious, and shows how the pursuit of Artificial Intelligence has helped us to appreciate how human and animal minds are possible.},
	language = {eng},
	number = {575},
	publisher = {Oxford University Press},
	author = {Boden, Margaret A.},
	year = {2018},
	note = {OCLC: 1050938367},
	keywords = {Artificial intelligence, Artificial Intelligence},
}

@book{mitchell_machine_2013,
	address = {New York},
	edition = {Nachdr.},
	series = {{McGraw}-{Hill} series in {Computer} {Science}},
	title = {Machine learning},
	isbn = {9780070428072 9780071154673},
	language = {eng},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {2013},
}

@book{rezende_sistemas_2003,
	address = {Barueri, SP},
	edition = {1. ed},
	title = {Sistemas inteligentes: fundamentos e aplicações},
	isbn = {9788520416839},
	shorttitle = {Sistemas inteligentes},
	language = {por},
	publisher = {Ed. Manole},
	editor = {Rezende, Solange Oliveira},
	year = {2003},
}

@book{sutton_reinforcement_2020,
	address = {Cambridge, Massachusetts London, England},
	edition = {Second edition},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {9780262039246},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {eng},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2020},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03762},
	doi = {10.48550/ARXIV.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-06-12},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{mahoney_framework_2019,
	title = {A {Framework} for {Explainable} {Text} {Classification} in {Legal} {Document} {Review}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1912.09501},
	doi = {10.48550/ARXIV.1912.09501},
	abstract = {Companies regularly spend millions of dollars producing electronically-stored documents in legal matters. Recently, parties on both sides of the 'legal aisle' are accepting the use of machine learning techniques like text classification to cull massive volumes of data and to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs in legal matters, it also faces a peculiar perception challenge: amongst lawyers, this technology is sometimes looked upon as a "black box", little information provided for attorneys to understand why documents are classified as responsive. In recent years, a group of AI and ML researchers have been actively researching Explainable AI, in which actions or decisions are human understandable. In legal document review scenarios, a document can be identified as responsive, if one or more of its text snippets are deemed responsive. In these scenarios, if text classification can be used to locate these snippets, then attorneys could easily evaluate the model's classification decision. When deployed with defined and explainable results, text classification can drastically enhance overall quality and speed of the review process by reducing the review time. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. This paper describes a framework for explainable text classification as a valuable tool in legal services: for enhancing the quality and efficiency of legal document review and for assisting in locating responsive snippets within responsive documents. This framework has been implemented in our legal analytics product, which has been used in hundreds of legal matters. We also report our experimental results using the data from an actual legal matter that used this type of document review.},
	urldate = {2025-06-12},
	publisher = {arXiv},
	author = {Mahoney, Christian J. and Zhang, Jianping and Huber-Fliflet, Nathaniel and Gronvall, Peter and Zhao, Haozhen},
	year = {2019},
	keywords = {Information Retrieval (cs.IR), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{brasil_lei_2011,
	title = {{LEI} {Nº} 12.527, {DE} 18 {DE} {NOVEMBRO} {DE} 2011},
	url = {https://www.planalto.gov.br/ccivil_03/_ato2011-2014/2011/lei/l12527.htm},
	language = {pt},
	author = {Brasil},
	month = nov,
	year = {2011},
}

@misc{brasil_lei_2009,
	title = {{LEI} {COMPLEMENTAR} {Nº} 131, {DE} 27 {DE} {MAIO} {DE} 2009},
	url = {https://www.planalto.gov.br/ccivil_03/leis/lcp/lcp131.htm},
	language = {pt},
	urldate = {2025-04-23},
	author = {Brasil},
	month = may,
	year = {2009},
}

@article{turing_i.computing_1950,
	title = {I.—{COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}},
	volume = {LIX},
	issn = {1460-2113, 0026-4423},
	url = {https://academic.oup.com/mind/article/LIX/236/433/986238},
	doi = {10.1093/mind/LIX.236.433},
	language = {en},
	number = {236},
	urldate = {2025-06-12},
	journal = {Mind},
	author = {Turing, A. M.},
	month = oct,
	year = {1950},
	pages = {433--460},
}

@techreport{mccarthy_proposal_1955,
	title = {A {Proposal} for the {Dartmouth} {Summer} {Research} {Project} on {Artificial} {Intelligence}},
	language = {English},
	institution = {Dartmouth College},
	author = {McCarthy, John and Minsky, Marvin and Rochester, Nathaniel and Shannon, Claude},
	year = {1955},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {9780262035613},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
}

@book{kandel_principles_2013,
	address = {New York},
	edition = {5th ed},
	title = {Principles of neural science},
	isbn = {9780071390118},
	abstract = {"The field's definitive work from a Nobel Prize-winning author 900 full-color illustrations Principles of Neural Science, 5e describes our current understanding of how the nerves, brain, and mind function. From molecules to anatomic structures and systems to cognitive function, this comprehensive reference covers all aspects of neuroscience. Widely regarded as the field's cornerstone reference, the fifth edition is highlighted by more than 900 full-color illustrations. The fifth edition has been completely updated to reflect the tremendous amount of new research and development in neuroscience in the last decade. Lead author Eric Kandel was awarded the Nobel Prize in Physiology or Medicine in 2000"--Provided by publisher},
	publisher = {McGraw-Hill},
	editor = {Kandel, Eric R.},
	year = {2013},
	keywords = {Central Nervous System, physiology, Mental Processes, physiology, Nervous System Diseases, Neuropsychology},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	copyright = {http://www.springer.com/tdm},
	issn = {0007-4985, 1522-9602},
	url = {http://link.springer.com/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	language = {en},
	number = {4},
	urldate = {2025-06-13},
	journal = {The Bulletin of Mathematical Biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	pages = {115--133},
}

@book{searle_rediscovery_2005,
	address = {Cambridge, MA},
	edition = {11. printing},
	series = {Representation and mind},
	title = {The rediscovery of the mind},
	isbn = {9780262193214 9780262691543},
	language = {eng},
	publisher = {MIT Press},
	author = {Searle, John R.},
	year = {2005},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {9780387848570 9780387848587},
	shorttitle = {The elements of statistical learning},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
	year = {2009},
	keywords = {Machine learning, Statistics, Methodology, Data mining, Bioinformatics, Inference, Forecasting, Computational intelligence},
}

@misc{devlin_bert:_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{BERT}},
	url = {https://arxiv.org/abs/1810.04805},
	doi = {10.48550/ARXIV.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-06-13},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@article{rosenblatt_perceptron:_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {https://doi.apa.org/doi/10.1037/h0042519},
	doi = {10.1037/h0042519},
	language = {en},
	number = {6},
	urldate = {2025-06-13},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {386--408},
}

@book{moravec_mind_1995,
	address = {Cambridge},
	edition = {4. print},
	title = {Mind children: the future of robot and human intelligence},
	isbn = {9780674576186},
	shorttitle = {Mind children},
	language = {eng},
	publisher = {Harvard Univ. Press},
	author = {Moravec, Hans},
	year = {1995},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	language = {en},
	number = {5},
	urldate = {2025-06-13},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	pages = {359--366},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	url = {https://api.semanticscholar.org/CorpusID:205001834},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year = {1986},
	pages = {533--536},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2025-06-13},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2025-06-13},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2025-06-13},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@book{jurafsky_speech_2009,
	address = {Upper Saddle River, NJ},
	edition = {2. ed. [Nachdr.]},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {9780131873216},
	shorttitle = {Speech and language processing},
	language = {eng},
	publisher = {Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	year = {2009},
}

@article{nadkarni_natural_2011,
	title = {Natural language processing: an introduction},
	volume = {18},
	issn = {1067-5027, 1527-974X},
	shorttitle = {Natural language processing},
	url = {https://academic.oup.com/jamia/article-lookup/doi/10.1136/amiajnl-2011-000464},
	doi = {10.1136/amiajnl-2011-000464},
	language = {en},
	number = {5},
	urldate = {2025-06-13},
	journal = {Journal of the American Medical Informatics Association},
	author = {Nadkarni, Prakash M and Ohno-Machado, Lucila and Chapman, Wendy W},
	month = sep,
	year = {2011},
	pages = {544--551},
}

@incollection{yang_deep_2023,
	title = {Deep {Learning} for {Natural} {Language} {Processing}},
	volume = {18},
	copyright = {https://creativecommons.org/licenses/by/3.0/legalcode},
	isbn = {9781803569505 9781803569512},
	url = {https://www.intechopen.com/chapters/87918},
	abstract = {With the constantly growing number of topical or sentiment-bearing texts and dialogs on the Web, the demand for automatic language or text analysis algorithms continues to expand. This chapter discusses about advanced deep learning techniques for classical and hot research directions in the field of natural language processing, including text classification, sentiment analysis, and task-oriented dialog systems. In text classification, we focus on tasks of multi-label text classification and extreme multi-label text classification, which allow for automatically annotates the texts with the most relevant labels. In sentiment analysis, we look into aspect-based sentiment analysis that makes automatic extraction of fine-grained sentiment information from texts, and multimodal sentiment analysis that classifies people’s opinions or attitudes from multimedia data through fusion techniques. In dialog system, we introduce how deep learning techniques work in pipeline mode and end-to-end mode for task-oriented dialog system. In this chapter, the rapidly evolving state of the research on the three topics is reviewed. Furthermore, trends in the research on deep learning for natural language processing are identified, and a discussion about future advances is provided.},
	language = {en},
	urldate = {2025-06-13},
	booktitle = {Artificial {Intelligence}},
	publisher = {IntechOpen},
	author = {Wang, Yuan and Li, Zekun and Deng, Zhenyu and Song, Huiling and Yang, Jucheng},
	editor = {Yang, Jucheng and Chen, Yarui and Zhao, Tingting and Wang, Yuan and Pan, Xuran},
	month = nov,
	year = {2023},
	doi = {10.5772/intechopen.112550},
}

@article{raiaan_review_2024,
	title = {A {Review} on {Large} {Language} {Models}: {Architectures}, {Applications}, {Taxonomies}, {Open} {Issues} and {Challenges}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	shorttitle = {A {Review} on {Large} {Language} {Models}},
	url = {https://ieeexplore.ieee.org/document/10433480/},
	doi = {10.1109/ACCESS.2024.3365742},
	urldate = {2025-06-13},
	journal = {IEEE Access},
	author = {Raiaan, Mohaimenul Azam Khan and Mukta, Md. Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
	year = {2024},
	pages = {26839--26874},
}

@misc{ansar_survey_2024,
	title = {A {Survey} on {Transformers} in {NLP} with {Focus} on {Efficiency}},
	copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
	url = {https://arxiv.org/abs/2406.16893},
	doi = {10.48550/ARXIV.2406.16893},
	abstract = {The advent of transformers with attention mechanisms and associated pre-trained models have revolutionized the field of Natural Language Processing (NLP). However, such models are resource-intensive due to highly complex architecture. This limits their application to resource-constrained environments. While choosing an appropriate NLP model, a major trade-off exists over choosing accuracy over efficiency and vice versa. This paper presents a commentary on the evolution of NLP and its applications with emphasis on their accuracy as-well-as efficiency. Following this, a survey of research contributions towards enhancing the efficiency of transformer-based models at various stages of model development along with hardware considerations has been conducted. The goal of this survey is to determine how current NLP techniques contribute towards a sustainable society and to establish a foundation for future research.},
	urldate = {2025-06-13},
	publisher = {arXiv},
	author = {Ansar, Wazib and Goswami, Saptarsi and Chakrabarti, Amlan},
	year = {2024},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{topal_exploring_2021,
	title = {Exploring {Transformers} in {Natural} {Language} {Generation}: {GPT}, {BERT}, and {XLNet}},
	shorttitle = {Exploring {Transformers} in {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/2102.08036},
	doi = {10.48550/arXiv.2102.08036},
	abstract = {Recent years have seen a proliferation of attention mechanisms and the rise of Transformers in Natural Language Generation (NLG). Previously, state-of-the-art NLG architectures such as RNN and LSTM ran into vanishing gradient problems; as sentences grew larger, distance between positions remained linear, and sequential computation hindered parallelization since sentences were processed word by word. Transformers usher in a new era. In this paper, we explore three major Transformer-based models, namely GPT, BERT, and XLNet, that carry significant implications for the field. NLG is a burgeoning area that is now bolstered with rapid developments in attention mechanisms. From poetry generation to summarization, text generation derives benefit as Transformer-based language models achieve groundbreaking results.},
	urldate = {2025-06-14},
	publisher = {arXiv},
	author = {Topal, M. Onat and Bas, Anil and Heerden, Imke van},
	month = feb,
	year = {2021},
	note = {arXiv:2102.08036},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{mandelbaum_word_2016,
	title = {Word {Embeddings} and {Their} {Use} {In} {Sentence} {Classification} {Tasks}},
	url = {http://arxiv.org/abs/1610.08229},
	doi = {10.48550/arXiv.1610.08229},
	abstract = {This paper have two parts. In the first part we discuss word embeddings. We discuss the need for them, some of the methods to create them, and some of their interesting properties. We also compare them to image embeddings and see how word embedding and image embedding can be combined to perform different tasks. In the second part we implement a convolutional neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classification tasks, and achieves state-of-art (or comparable) results, demonstrating the great power of pre-trainted word embeddings over random ones.},
	urldate = {2025-06-14},
	publisher = {arXiv},
	author = {Mandelbaum, Amit and Shalev, Adi},
	month = oct,
	year = {2016},
	note = {arXiv:1610.08229},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	doi = {10.48550/arXiv.1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2025-06-14},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv:1310.4546 
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{pennington_glove:_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	language = {en},
	urldate = {2025-06-14},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
}

@misc{fu_decoder-only_2023,
	title = {Decoder-{Only} or {Encoder}-{Decoder}? {Interpreting} {Language} {Model} as a {Regularized} {Encoder}-{Decoder}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Decoder-{Only} or {Encoder}-{Decoder}?},
	url = {https://arxiv.org/abs/2304.04052},
	doi = {10.48550/ARXIV.2304.04052},
	abstract = {The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.},
	urldate = {2025-06-14},
	publisher = {arXiv},
	author = {Fu, Zihao and Lam, Wai and Yu, Qian and So, Anthony Man-Cho and Hu, Shengding and Liu, Zhiyuan and Collier, Nigel},
	year = {2023},
	keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@inproceedings{wu_multimodal_2023,
	address = {Sorrento, Italy},
	title = {Multimodal {Large} {Language} {Models}: {A} {Survey}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350324457},
	shorttitle = {Multimodal {Large} {Language} {Models}},
	url = {https://ieeexplore.ieee.org/document/10386743/},
	doi = {10.1109/BigData59044.2023.10386743},
	urldate = {2025-06-15},
	booktitle = {2023 {IEEE} {International} {Conference} on {Big} {Data} ({BigData})},
	publisher = {IEEE},
	author = {Wu, Jiayang and Gan, Wensheng and Chen, Zefeng and Wan, Shicheng and Yu, Philip S.},
	month = dec,
	year = {2023},
	pages = {2247--2256},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2005.14165},
	doi = {10.48550/ARXIV.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2025-06-15},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{xu_contrastive_2024,
	title = {Contrastive {Preference} {Optimization}: {Pushing} the {Boundaries} of {LLM} {Performance} in {Machine} {Translation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Contrastive {Preference} {Optimization}},
	url = {https://arxiv.org/abs/2401.08417},
	doi = {10.48550/ARXIV.2401.08417},
	abstract = {Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.},
	urldate = {2025-06-15},
	publisher = {arXiv},
	author = {Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and Shen, Lingfeng and Van Durme, Benjamin and Murray, Kenton and Kim, Young Jin},
	year = {2024},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{balestriero_cookbook_2023,
	title = {A {Cookbook} of {Self}-{Supervised} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2304.12210},
	doi = {10.48550/ARXIV.2304.12210},
	abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
	urldate = {2025-06-15},
	publisher = {arXiv},
	author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
	year = {2023},
	keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2206.07682},
	doi = {10.48550/ARXIV.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2025-06-15},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	year = {2022},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@article{ferreira_redacao_2015,
	title = {Redação {Oficial}},
	url = {http://educapes.capes.gov.br/handle/capes/145384},
	abstract = {Material didático da disciplina Redação Oficial (Optativa) - Bacharelado em Administração Pública - Programa Nacional de Formação em Administração Pública (PNAP)},
	language = {pt\_BR},
	urldate = {2025-06-15},
	author = {Ferreira, Eric Duarte and Cambrussi, Morgana Fabiola},
	year = {2015},
}

@book{republica_manual_2018,
	title = {Manual de redação da {Presidência} da {República}},
	isbn = {9788585142964},
	language = {pt-BR},
	publisher = {Presidência da República},
	author = {República, Presidência da},
	month = dec,
	year = {2018},
	keywords = {Linguagem e línguas},
}

@misc{brasil_lei_1991,
	title = {Lei nº 8.159, de 8 de janeiro de 1991},
	url = {https://www.planalto.gov.br/ccivil_03/leis/l8159.htm},
	language = {pt-br},
	author = {Brasil},
	month = jan,
	year = {1991},
}

@article{fernandes_propaganda_2021,
	title = {{PROPAGANDA}, {TRANSPARÊNCIA} {E} {ACCOUNTABILITY}: {A} {CONSTRUÇÃO} {DE} {INDICADORES} {PARA} {UMA} {GOVERNANÇA} {DEMOCRÁTICA}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0},
	issn = {2237-1087},
	shorttitle = {{PROPAGANDA}, {TRANSPARÊNCIA} {E} {ACCOUNTABILITY}},
	url = {http://seer.pucgoias.edu.br/index.php/panorama/article/view/9026},
	doi = {10.18224/pan.v11i1.9026},
	abstract = {Este artigo tem por objetivo analisar as questões que envolvem a transparência, as fakes news e a accountability. Entende-se que a accountability pode não apenas melhorar a qualidade da democracia e da cidadania, como também possibilitar construir uma fórmula de governança democrática que ajude a melhorar a vida do cidadão e a medir o grau de corrupção, possibilitando criar uma forma concreta de combatê-la. Mas uma democracia com cidadania forte deve não apenas incorporar, em seu arcabouço legal, as noções e obrigações da transparência e do acesso à informação de interesse público, como também buscar construir mecanismos jurídicos para assegurar a accountability em todas as suas formas e direções.},
	number = {1},
	urldate = {2025-06-15},
	journal = {Revista Panorama - Revista de Comunicação Social},
	author = {Fernandes, Luiz Carlos Do Carmo},
	month = sep,
	year = {2021},
	pages = {46},
}

@misc{ollama__ollama_nodate,
	title = {Ollama},
	url = {https://ollama.com/},
	author = {Ollama },
}

@article{al-banna_natural_2022,
	title = {Natural {Language} {Processing} {For} {Automatic} text summarization [{Datasets}] - {Survey}},
	volume = {1},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2788-5879, 2788-5887},
	url = {http://wjcm.uowasit.edu.iq/index.php/wjcm/article/view/72},
	doi = {10.31185/wjcm.72},
	abstract = {Natural language processing has developed significantly recently, which has progressed the text summarization task. It is no longer limited to reducing the text size or obtaining helpful information from a long document only. It has begun to be used in getting answers from summarization, measuring the quality of sentiment analysis systems, research and mining techniques, document categorization, and natural language Inference, which increased the importance of scientific research to get a good summary. This paper reviews the most used datasets in text summarization in different languages and types, with the most effective methods for each dataset. The results are shown using text summarization matrices. The review indicates that the pre-training models achieved the highest results in the summary measures in most of the researchers' works for the datasets. Dataset English made up about 75\% of the databases available to researchers due to the extensive use of the English language. Other languages such as Arabic, Hindi, and others suffered from low resources of dataset sources, which limited progress in the academic field.},
	number = {4},
	urldate = {2025-06-15},
	journal = {Wasit Journal of Computer and Mathematics Science},
	author = {AL-Banna, Alaa Ahmed and AL-Mashhadany, Abeer K.},
	month = dec,
	year = {2022},
	pages = {102--110},
}

@misc{katz_natural_2023,
	title = {Natural {Language} {Processing} in the {Legal} {Domain}},
	url = {http://arxiv.org/abs/2302.12039},
	doi = {10.48550/arXiv.2302.12039},
	abstract = {In this paper, we summarize the current state of the field of NLP \& Law with a specific focus on recent technical and substantive developments. To support our analysis, we construct and analyze a nearly complete corpus of more than six hundred NLP \& Law related papers published over the past decade. Our analysis highlights several major trends. Namely, we document an increasing number of papers written, tasks undertaken, and languages covered over the course of the past decade. We observe an increase in the sophistication of the methods which researchers deployed in this applied context. Slowly but surely, Legal NLP is beginning to match not only the methodological sophistication of general NLP but also the professional standards of data availability and code reproducibility observed within the broader scientific community. We believe all of these trends bode well for the future of the field, but many questions in both the academic and commercial sphere still remain open.},
	urldate = {2025-06-15},
	publisher = {arXiv},
	author = {Katz, Daniel Martin and Hartung, Dirk and Gerlach, Lauritz and Jana, Abhik and II, Michael J. Bommarito},
	month = feb,
	year = {2023},
	note = {arXiv:2302.12039},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}


@article{yin_survey_2023,
    author = {Yin, Shijie and others},
    title = {A Survey on Multimodal Large Language Models},
    journal = {arXiv preprint arXiv:2306.13549},
    year = {2023},
    url = {https://arxiv.org/pdf/2306.13549},
    urldate = {2025-11-08},
    note = {Disponível em: <https://arxiv.org/pdf/2306.13549>. Acesso em: 8 nov. 2025.}
}


@misc{schmidt_recurrent_2019,
	title = {Recurrent {Neural} {Networks} ({RNNs}): {A} gentle {Introduction} and {Overview}},
	shorttitle = {Recurrent {Neural} {Networks} ({RNNs})},
	url = {http://arxiv.org/abs/1912.05911},
	doi = {10.48550/arXiv.1912.05911},
	abstract = {State-of-the-art solutions in the areas of "Language Modelling \& Generating Text", "Speech Recognition", "Generating Image Descriptions" or "Video Tagging" have been using Recurrent Neural Networks as the foundation for their approaches. Understanding the underlying concepts is therefore of tremendous importance if we want to keep up with recent or upcoming publications in those areas. In this work we give a short overview over some of the most important concepts in the realm of Recurrent Neural Networks which enables readers to easily understand the fundamentals such as but not limited to "Backpropagation through Time" or "Long Short-Term Memory Units" as well as some of the more recent advances like the "Attention Mechanism" or "Pointer Networks". We also give recommendations for further reading regarding more complex topics where it is necessary.},
	urldate = {2025-11-09},
	publisher = {arXiv},
	author = {Schmidt, Robin M.},
	month = nov,
	year = {2019},
	note = {arXiv:1912.05911},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{sherstinsky_fundamentals_2023,
	title = {Fundamentals of {Recurrent} {Neural} {Network} ({RNN}) and {Long} {Short}-{Term} {Memory} ({LSTM}) {Network}},
	url = {http://arxiv.org/abs/1808.03314},
	doi = {10.48550/arXiv.1808.03314},
	abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well.},
	urldate = {2025-11-09},
	publisher = {arXiv},
	author = {Sherstinsky, Alex},
	month = jul,
	year = {2023},
	note = {arXiv:1808.03314},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{vennerod_long_2021,
	title = {Long {Short}-term {Memory} {RNN}},
	url = {http://arxiv.org/abs/2105.06756},
	doi = {10.48550/arXiv.2105.06756},
	abstract = {This paper is based on a machine learning project at the Norwegian University of Science and Technology, fall 2020. The project was initiated with a literature review on the latest developments within time-series forecasting methods in the scientific community over the past five years. The paper summarizes the essential aspects of this research. Furthermore, in this paper, we introduce an LSTM cell's architecture, and explain how different components go together to alter the cell's memory and predict the output. Also, the paper provides the necessary formulas and foundations to calculate a forward iteration through an LSTM. Then, the paper refers to some practical applications and research that emphasize the strength and weaknesses of LSTMs, shown within the time-series domain and the natural language processing (NLP) domain. Finally, alternative statistical methods for time series predictions are highlighted, where the paper outline ARIMA and exponential smoothing. Nevertheless, as LSTMs can be viewed as a complex architecture, the paper assumes that the reader has some knowledge of essential machine learning aspects, such as the multi-layer perceptron, activation functions, overfitting, backpropagation, bias, over- and underfitting, and more.},
	urldate = {2025-11-09},
	publisher = {arXiv},
	author = {Vennerød, Christian Bakke and Kjærran, Adrian and Bugge, Erling Stray},
	month = may,
	year = {2021},
	note = {arXiv:2105.06756},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{le_quantifying_2016,
	title = {Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive {LSTMs}},
	url = {http://arxiv.org/abs/1603.00423},
	doi = {10.48550/arXiv.1603.00423},
	abstract = {Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.},
	urldate = {2025-11-09},
	publisher = {arXiv},
	author = {Le, Phong and Zuidema, Willem},
	month = mar,
	year = {2016},
	note = {arXiv:1603.00423},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2025-11-09},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@misc{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1406.1078},
	doi = {10.48550/ARXIV.1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2025-11-09},
	publisher = {arXiv},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	year = {2014},
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@inproceedings{Plaquet23,
  author={Alexis Plaquet and Hervé Bredin},
  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}